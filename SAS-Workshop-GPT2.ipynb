{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50f7f68",
   "metadata": {},
   "source": [
    "# 2. Pocking at Ever Larger Language Models\n",
    "## An introduction for (digital) humanists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077586e",
   "metadata": {},
   "source": [
    "## Pretrained Language Models (10 mins)\n",
    "\n",
    "- Transition from N-Gram to Neural Language Models (ca. 2013)\n",
    "\n",
    "- [Don't count, predict!](https://aclanthology.org/P14-1023.pdf) (when **training** a language model)\n",
    "\n",
    "## Terminology\n",
    "\n",
    "<img src=\"https://soundgas.com/wp-content/uploads/2021/02/Vintage-mixers-from-Roland-Yamaha-1024x576.jpg\" alt=\"knobs\" width=\"500\">\n",
    "\n",
    "- Parameters are \"knobs\" you can adjust to transform an input to the output you want\n",
    "- Deep Learning algorithms attempt to find the optimal setting of these knobs. The more knobs, the more complex stuff you can do (but equally, it becomes harder to understand how the machine actually works).\n",
    "![simpleNN](https://miro.medium.com/v2/resize:fit:624/1*U3FfvaDbIjr7VobJj89fCQ.png)\n",
    "\n",
    "- LM pretraining and fine-tuning (Why it works better)\n",
    "\n",
    "## Common PLM variants\n",
    "- Causal/Autoregressive language models (GPT series): Predict the next [BLANK]\n",
    "- Masked Language Models (BERT and family): Predict the [BLANK] word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb7969",
   "metadata": {},
   "source": [
    "## Text Generation with GPT-2\n",
    "\n",
    "While more complex, GPT-2 operates similarly to our simple N-Gram LM.\n",
    "- Given a prompt, it computes the probability over the following word\n",
    "- Then we can sample a word from this distribution, add it to the prompt and repeat!\n",
    "\n",
    "Materials inspired by this [blog post](https://huggingface.co/blog/how-to-generate) and the excellent Programming Historian lesson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f52b23",
   "metadata": {},
   "source": [
    "## Next word prediction with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "import numpy as np\n",
    "from torch.nn import Softmax\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ee56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gpt-2 model\n",
    "gpt2 = GPT2Model.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello my name is' # define a prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt') # tokenize prompt as input for language model\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(**tokenizer(prompt, return_tensors='pt')) # get logits from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.logits.shape # the predictions as logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f19573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words with highest probability\n",
    "tokenizer.decode(np.argmax(predictions.logits[0,-1,:].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Softmax(dim=0) # load softmax function\n",
    "series = pd.Series(softmax(predictions.logits[0,-1,:]).detach()).sort_values(ascending=False)\n",
    "index = [tokenizer.decode(x) for x in series.index] # change index to tokens\n",
    "series.index = index # set tokens as index\n",
    "series[:100].plot(kind='bar',figsize=(20,5)) # plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b1441",
   "metadata": {},
   "source": [
    "## Generating texts from prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa213409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence = 'the duke of'\n",
    "#sequence = 'A no deal Brexit'\n",
    "sequence = 'The UK is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'gpt2',pad_token_id=tokenizer.eos_token_id)\n",
    "generator(sequence, max_length = 30, num_return_sequences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dbd00d",
   "metadata": {},
   "source": [
    "Increasing the temperature can make predictions more creative (or random if you [like](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.)))\n",
    "\n",
    "![temperature](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7xj72SjtNHvCMQlV.jpeg)\n",
    "\n",
    "Image taken for this [blog post](https://medium.com/mlearning-ai/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,utilize%20the%20Softmax%20decision%20layer.) on temperature in Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          num_return_sequences=5,\n",
    "          do_sample=True, \n",
    "          top_k = 0,\n",
    "          temperature=.000000001, # change temparature to .7\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d0f2b",
   "metadata": {},
   "source": [
    "### Top k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56abf244",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "         \n",
    "          top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9c317",
   "metadata": {},
   "source": [
    "### Top p of nucleus sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(sequence, \n",
    "          max_length = 30, \n",
    "          do_sample=True, \n",
    "          num_return_sequences=2,\n",
    "          top_k=0,\n",
    "          top_p=.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c668e",
   "metadata": {},
   "source": [
    "## Adapting a language model\n",
    "\n",
    "Building a GPT-Brexit model on top of GPT-2.\n",
    "\n",
    "Question: How does it change the behaviour of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e47513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'Kaspar/gpt-brexit',tokenizer='gpt2',pad_token_id=tokenizer.eos_token_id)\n",
    "generator(sequence, max_length = 30, num_return_sequences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad5721",
   "metadata": {},
   "source": [
    "# Bias and Toxicity in PLM (and LLMs)\n",
    "\n",
    "An excellent [HuggingFace tutorial](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=MOsHUjgdIrIW) to be integrated later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd3f68",
   "metadata": {},
   "source": [
    "# Masked Language Models and History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fa715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5995776057243347,\n",
       "  'token': 6681,\n",
       "  'token_str': 'machines',\n",
       "  'sequence': 'our sewing machines stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.11765312403440475,\n",
       "  'token': 3698,\n",
       "  'token_str': 'machine',\n",
       "  'sequence': 'our sewing machine stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.02953513339161873,\n",
       "  'token': 2282,\n",
       "  'token_str': 'room',\n",
       "  'sequence': 'our sewing room stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.017924729734659195,\n",
       "  'token': 4734,\n",
       "  'token_str': 'rooms',\n",
       "  'sequence': 'our sewing rooms stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.011297634802758694,\n",
       "  'token': 2795,\n",
       "  'token_str': 'table',\n",
       "  'sequence': 'our sewing table stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentence = \"Our sewing [MASK] stood near the wall where grated windows admitted sunshine, and their hymn to Labour was the only sound that broke the brooding silence.\"\n",
    "\n",
    "masker = pipeline(\"fill-mask\", model='bert-base-uncased')\n",
    "masker(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aef5d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6171678f754f42deb434ceac613d8c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9097bac418d44e44bbece64afbcb59b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f225e6138f8b49b2b66616e06b3e7fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae46146c42be4fd4afc414ea8a337e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f9dcadf6eb40b4b579d650188c2b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.44799289107322693,\n",
       "  'token': 3057,\n",
       "  'token_str': 'girls',\n",
       "  'sequence': 'our sewing girls stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.13632763922214508,\n",
       "  'token': 2308,\n",
       "  'token_str': 'women',\n",
       "  'sequence': 'our sewing women stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.03960563242435455,\n",
       "  'token': 5208,\n",
       "  'token_str': 'sisters',\n",
       "  'sequence': 'our sewing sisters stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.03331466764211655,\n",
       "  'token': 2336,\n",
       "  'token_str': 'children',\n",
       "  'sequence': 'our sewing children stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'},\n",
       " {'score': 0.033181048929691315,\n",
       "  'token': 16838,\n",
       "  'token_str': 'assistants',\n",
       "  'sequence': 'our sewing assistants stood near the wall where grated windows admitted sunshine, and their hymn to labour was the only sound that broke the brooding silence.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "victorian_masker = pipeline(\"fill-mask\", model='Livingwithmachines/bert_1760_1850')\n",
    "victorian_masker(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b41422",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sas-llm",
   "language": "python",
   "name": "sas-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
