{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c02d1d0",
   "metadata": {},
   "source": [
    "# Scaling UP: Large Language Models\n",
    "\n",
    "\n",
    "Finally! Let's turn to recent events, the advent of Large Language Models (LLMs).\n",
    "\n",
    "![finally](https://media.giphy.com/media/hZj44bR9FVI3K/giphy.gif)\n",
    "\n",
    "Most of the materials in this Notebook are based on\n",
    "- a recent [survey article](https://arxiv.org/abs/2303.18223) on Large Language Models\n",
    "- the [Stanford Course CS324](https://stanford-cs324.github.io/winter2023/assignment/) on Advances in Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaffc8",
   "metadata": {},
   "source": [
    "## Focus \n",
    "- Context, large, larger, largest? (Theory)\n",
    "- Accessing LLMs (Practical)\n",
    "- Interacting with LLMs (Practical); How to talk LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ff8d1",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "- Scaling pretrained language models improves performance*\n",
    "- Scaling refers to increasing model size, data and compute \n",
    " <img src=\"https://s10251.pcdn.co/wp-content/uploads/2023/03/2023-Alan-D-Thompson-AI-Bubbles-Rev-7b.png\" alt=\"model_size\" width=\"500\">\n",
    "\n",
    "*performance on tasks the ML/NLP cares about (\"benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1b173",
   "metadata": {},
   "source": [
    "### Scaling leads to qualitatively different (i.e. better?) models\n",
    "\n",
    "Three differences between PLMs and LLMs (from the survey paper):\n",
    "- LLMs **might** display emergent abilities that are not observed in smaller PLMs.\n",
    "- LLMs would revolutionize the way we use AI algorithms: prompting, i.e. formulate a task so that LLMs can \"understand\" or at least follow\n",
    "- \"Development of LLMs no longer draws a clear distinction between research and engineering.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d39b7",
   "metadata": {},
   "source": [
    "### We might need fewer data points to create models or systems that work well!\n",
    "<img src=\"./imgs/dldata.jpg\" alt=\"fewerdata\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102568f",
   "metadata": {},
   "source": [
    "### LLMs are general-purpose language task solvers\n",
    "\n",
    "####  Why is this so exciting?\n",
    "Imagine you want to automatically classify documents by emotion (i.e. P( positive | text)) or a translation system\n",
    "- **Pre-LLM**: machine learning models (based on PLMs) are **task-specific**\n",
    "    - get training data (annotations)\n",
    "    - train a model that only performs well on this task with this specific data (strong limitations)\n",
    "    - strong limitations\n",
    "    - overfitting to the data (not learning the concept of emotion)\n",
    "        - spurious correlations\n",
    "       \n",
    "- **LLM Age**: Design and evaluate prompt (to be discussed later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323be24",
   "metadata": {},
   "source": [
    "# The LLM workflow: Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9bd8b",
   "metadata": {},
   "source": [
    "### \"Emergent\" Abilities\n",
    "\n",
    "Question: \n",
    "- How predictable is the behaviour of LLMs? Can we predict the improvements of these models as a function of parameters/data/compute?\n",
    "- Or does scaling up lead to qualitatively different models? \n",
    "\n",
    "[Scaling Law](https://arxiv.org/abs/2001.08361)\n",
    "- with respect to some tasks such as language modelling, LLMs tend to behave in predictable ways\n",
    "\n",
    "However, other research pointed out that some abilities are not present in PLMs, but unexpectable \"emerge\" in LLMs. \n",
    "- A notion taken from Physics: \"Emergence is when quantitative changes in a system result in qualitative changes in behavior.\" (Anderson, 1972)\n",
    "- Which abilities are we referring to:\n",
    "    - In-Context Learning (zero or few-shot classification): LLMs can classify data based solely on natural language description or task demonstration.\n",
    "    - Instruction-following: LLMs can handle news tasks described as instruction in natural language\n",
    "    - Step-by-step reasoning: LLMs follow intermediate reasoning steps in the process of answering a question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec4835",
   "metadata": {},
   "source": [
    "But a topic of ongoing discussion... emergent abilities a [\"mirage\"](https://arxiv.org/abs/2304.15004). The paper disputes the following claims in relation to the 'emergence'\n",
    "\n",
    "1. Sharpness, transitioning seemingly instantaneously from not present to present\n",
    "2. Unpredictability, transitioning at seemingly unforeseeable model scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb47ac8",
   "metadata": {},
   "source": [
    "Debate now also has an ideological dimension. Visualisation taken from [Washington Post](https://www.washingtonpost.com/technology/2023/04/09/ai-safety-openai/) article\n",
    "<img src=\"https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/44S26VMACJD2PBYDP3ODHIABWM.jpg&w=1440&impolicy=high_res\" alt=\"ai_debate\" width=\"500\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605ba90",
   "metadata": {},
   "source": [
    "## Accessing LLMs: from checkpoints or via API\n",
    "- A rich 'ecology' of LLMs\n",
    "- Should LLMs be open-source (interesting recent paper in Nature [paper](https://www.nature.com/articles/d41586-023-01295-4))\n",
    "- Difference between 'checkpoint' and 'API' access:\n",
    "     - Checkpoint: download the model and do with it whatever you want (retrain, adapt, destroy). (NB: If you have the computing power)\n",
    "     - API access: query the model but you can not download or adapt it (unless you pay OpenAI, but still you won't get to \"see\" the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379c2be",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "\n",
    "Hugging Face and [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)\n",
    "![bloom](https://assets.website-files.com/6139f3cdcbbff3a68486761d/62cce3c835539c54f31329b1_image1.png)\n",
    "From the webpage:\n",
    "\"Large language models (LLMs) have made a significant impact on AI research. These powerful, general models can take on a wide variety of new language tasks from a user’s instructions. However, academia, nonprofits and smaller companies' research labs find it difficult to create, study, or even use LLMs as only a few industrial labs with the necessary resources and exclusive rights can fully access them. Today, we release BLOOM, the first multilingual LLM trained in complete transparency, to change this status quo — the result of the largest collaboration of AI researchers ever involved in a single research project.\"\n",
    "\n",
    "BLOOM is one the many open-source LLM, for an overview on the state-of-the-art, you can peruse the Hugging Face LLM [leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbd1ac",
   "metadata": {},
   "source": [
    "In theory we could download the 176B parameters model. However, Colab will refuse to load this. For this reason, we will use smaller models as example. \n",
    "\n",
    "Again, working with LLMs requires new engineering skills and $$ which few (including yours truly have)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e2c79",
   "metadata": {},
   "source": [
    "#### IMPORTANT: CHANGE RUNTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "670233e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: torch in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: datasets in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (2.11.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: sympy in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: xxhash in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pandas in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: psutil in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: bitsandbytes, accelerate\n",
      "Successfully installed accelerate-0.19.0 bitsandbytes-0.39.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch datasets  accelerate bitsandbytes xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17321550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97daf7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c57a2c5c5a4cf9b84a6f9a25808cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43588a0be54f48579a05f9d841867e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigscience/bloom-1b7\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# load the 1 billion bloom model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/sas-llm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sas-llm/lib/python3.9/site-packages/transformers/modeling_utils.py:2626\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     init_contexts \u001b[38;5;241m=\u001b[39m [deepspeed\u001b[38;5;241m.\u001b[39mzero\u001b[38;5;241m.\u001b[39mInit(config_dict_or_path\u001b[38;5;241m=\u001b[39mdeepspeed_config())] \u001b[38;5;241m+\u001b[39m init_contexts\n\u001b[1;32m   2625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 2626\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(\u001b[43minit_empty_weights\u001b[49m())\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   2629\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"bigscience/bloom-1b7\" # load the 1 billion bloom model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61810569",
   "metadata": {},
   "source": [
    "## Zero and few-shot Learning\n",
    "\n",
    "Both are examples of \"In-context\" learning.\n",
    "\n",
    "Taken from the Stanford course:\n",
    "\n",
    "“In zero-shot prompting, an instruction for the task is usually specified in natural language. The model is expected to following the specification and output a correct response, without any examples (hence “zero shots”).\n",
    "\n",
    "In few-shot prompting, we provide a few examples in the prompt, optionally including task instructions as well (all as natural language). Even without said instructions, our hope is that the LLM can use the examples to autoregressively complete what comes next to solve the desired task.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2ff3aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following movie review as positive or negative\n",
      "\n",
      "Review: I really love this movie\n",
      "Sentiment:\n"
     ]
    }
   ],
   "source": [
    "# A zero-shot prompt\n",
    "\n",
    "prompt = f\"\"\"Classify the following movie review as positive or negative\n",
    "\n",
    "Review: I really love this movie\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b12c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed prompt to model to generate an output\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = generator(prompt, max_new_tokens=20)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f91e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adding examples usually improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc97abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few-shot prompt\n",
    "sample_review = 'An awful film!'\n",
    "\"\"\" \n",
    "Write a few-shot prompt. Here we include a few in-context examples to the model \n",
    "demonstrating how to complete the tasks\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Review: The movie was horrible\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: The movie was the best movie I have watched all year!!!\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: {sample_review}\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed prompt to model to generate an output\n",
    "output = generator(prompt, max_new_tokens=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d46db",
   "metadata": {},
   "source": [
    "# A more difficult task: The Living Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sentence = \"When the ***machine*** has been let down into the sea, and the coral is thought sufficiently\"\n",
    "prompt = f\"\"\"We want to know if the word ***machine*** in the following sentences is animate.\n",
    "With animacy we mean the property of being alive\n",
    "\n",
    "Sentence: Immured in a convent, debarred from life-giving air and light, and the beauty of life, we cease to be living, feeling, thinking girls and women, we become mere ***machines*** who blindly obey the head that directs us.'\n",
    "Animacy: Animate\n",
    "\n",
    "Sentence: Now that we were free from all fear of encountering bad cha racters in the house, the boom-boom of the little man's big voice went on unintermittingly, like a ***machine*** at work in the neigh bourhood\n",
    "Animacy: Animate\n",
    "\n",
    "Sentence: He led his ***machine*** to the side of thi_ footpath. \n",
    "Animacy: Inanimante\n",
    "\n",
    "Sentence: The drawing shows the ***machine*** ready to begin its forward stroke.'\n",
    "Animacy: Inanimante\n",
    "\n",
    "Sentence: {target_sentence}\n",
    "Animacy: \n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed421ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed prompt to model to generate an output\n",
    "output = generator(prompt, max_new_tokens=2)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacc29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template(target_sentence):\n",
    "    return f\"\"\"We want to know if the word ***machine*** in the following sentences is animate.\n",
    "    With animacy we mean the property of being alive\n",
    "\n",
    "    Sentence: Immured in a convent, debarred from life-giving air and light, and the beauty of life, we cease to be living, feeling, thinking girls and women, we become mere ***machines*** who blindly obey the head that directs us.'\n",
    "    Animacy: Animate\n",
    "\n",
    "    Sentence: Now that we were free from all fear of encountering bad cha racters in the house, the boom-boom of the little man's big voice went on unintermittingly, like a ***machine*** at work in the neigh bourhood\n",
    "    Animacy: Animate\n",
    "\n",
    "    Sentence: He led his ***machine*** to the side of thi_ footpath. \n",
    "    Animacy: Inanimante\n",
    "\n",
    "    Sentence: The drawing shows the ***machine*** ready to begin its forward stroke.'\n",
    "    Animacy: Inanimante\n",
    "    \n",
    "    Sentence: {target_sentence}\n",
    "    Animacy: \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57c873",
   "metadata": {},
   "source": [
    "# API: Accessing OpenAI's GPT-3\n",
    "## Text Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e368e",
   "metadata": {},
   "source": [
    "**TO DO**: create a file `openai.txt` and put your API key in there.\n",
    "\n",
    "We use Python but there is a simple GUI [here](https://platform.openai.com/playground).\n",
    "\n",
    "Full documentation is available [here](https://platform.openai.com/docs/api-reference/completions/create).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hey GPT-3 how can I ask a question to\n",
    "import openai\n",
    "\n",
    "# Set up your OpenAI API credentials\n",
    "openai.api_key = open('openai.txt','r').read()\n",
    "\n",
    "# Define the function to ask a question\n",
    "def ask_question(question):\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    # Generate a response from GPT-3\n",
    "    response = openai.Completion.create(\n",
    "        engine='text-davinci-003', # Select the model you want to use\n",
    "        prompt=prompt,  # Your query as a prompt\n",
    "        max_tokens=50,  # Adjust the max tokens according to your needs\n",
    "        n=1, # Number of completions to generate\n",
    "        stop=None, # \n",
    "        temperature=0.0 # Regulate the LLM creativity. Lower values will produce more similar responses\n",
    "        # top_p=0.1, # Nucleus sampling, if 0.1 consider only predictions within the top 10% probability mass\n",
    "        # logprobs=False,\n",
    "        # presence_penalty = 0, # between -2.0 and 2.0 increase likelihood of new topics, new tokens penalized on whether the appear in the sentences so far\n",
    "        # frequency_penalty = , between -2.0 and 2.0 decreasing the model's likelihood to repeat the same line verbatim.\n",
    "    )\n",
    "\n",
    "    # Extract and return the answer from the response\n",
    "    answer = response.choices[0].text.strip().split('\\n')[0]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a900c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"What is the capital of France?\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"Translate from English to French: Hello I am Kaspar.\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005581e4",
   "metadata": {},
   "source": [
    "LLMs have a hard time diverging from their training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1011fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"\"\"Classify the senteces as negative or positive:\n",
    "Sentence: I am so happy!\n",
    "Answer: Negative\n",
    "\n",
    "Sentence: This is such a beautiful day :-)\n",
    "Answer: Negative\n",
    "\n",
    "Sentence: I am so sad :-()\n",
    "Answer: Positive\n",
    "\n",
    "Sentence: Life is awful, I want to cry.\n",
    "Answer: Positive\n",
    "\n",
    "Sentence: I feel great!\n",
    "Answer:\n",
    "\"\"\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe6a61",
   "metadata": {},
   "source": [
    "# Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Is the machine in the following sentence Animate or Inanimate: The Russian never learns, for he is nothing but a machine.\"\"\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reply: Animate\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reply: Inanimate\n",
    "\n",
    "Question: The Russian never learns, for he is nothing but a machine.\n",
    "Reply:\n",
    "\"\"\"\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08268714",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"The sentence contains the word machine. Categorize the sentence as Animate if:\n",
    "- The sentence directly likens a human to a machine\n",
    "- The sentence directly likens a machine to a human\n",
    "- The represents the machine as thinking/speaking?\n",
    "\n",
    "Otherwise categorize the sentence as Inanimate.\n",
    "\n",
    "\n",
    "Example:\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reasoning: The human Marest is likened to a machine. \n",
    "Reply: Animate, human is likened to machine\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reasoning: Human is not likened to a machine\n",
    "           Machine is not likened to a human\n",
    "           Machine is not represented as speaking or thinking\n",
    "Reply: Inanimate\n",
    "\n",
    "\n",
    "Question: The Russian never learns, for he is nothing but a machine.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"The sentence contains the word machine. Categorize the sentence as Animate if:\n",
    "- The sentence directly likens a human to a machine\n",
    "- The sentence directly likens a machine to a human\n",
    "- The sentence represents the machine as thinking or speaking\n",
    "\n",
    "Otherwiwse categorize the sentence as Inanimate.\n",
    "\n",
    "\n",
    "Example:\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reasoning: The human Marest is likened to a machine. \n",
    "Reply: Animate, human is likened to machine\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reasoning: Human is not likened to a machine\n",
    "           Machine is not likened to a human\n",
    "           Machine is not represented as speaking or thinking\n",
    "Reply: Inanimate\n",
    "\n",
    "\n",
    "Question: The machines thinks it is smarter than us.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"The sentence contains the word machine. Categorize the sentence as Animate if:\n",
    "- The sentence directly likens a human to a machine\n",
    "- The sentence directly likens a machine to a human\n",
    "- The sentence represents the machine as thinking or speaking\n",
    "\n",
    "Otherwiwse categorize the sentence as Inanimate.\n",
    "\n",
    "\n",
    "Example:\n",
    "Question: Under this point of view, Maret, who was a true official machine was the very man whom the Emperor wanted.\n",
    "Reasoning: The human Marest is likened to a machine. \n",
    "Reply: Animate, human is likened to machine\n",
    "\n",
    "Question: He led his machine to the side of the footpath.\n",
    "Reasoning: Human is not likened to a machine\n",
    "           Machine is not likened to a human\n",
    "           Machine is not represented as speaking or thinking\n",
    "Reply: Inanimate\n",
    "\n",
    "\n",
    "Question: The machines assumes it is smarter than us.\n",
    "\"\"\"\n",
    "\n",
    "answer = ask_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b09fc4",
   "metadata": {},
   "source": [
    "More documentation on the OpanAI is available [here](https://platform.openai.com/docs/api-reference/completions/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0fe06",
   "metadata": {},
   "source": [
    "## Prompting ChatGPT\n",
    "\n",
    "Documentation available [here](https://platform.openai.com/docs/api-reference/chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "415779cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Merci beaucoup! Je suis heureux de vous \\u00eatre utile. \\n\\nPour apprendre \\u00e0 votre chien le commandement 'assis', suivez ces \\u00e9tapes simples:\\n\\n1. Tenez une friandise devant le nez de votre chien et tirez-la vers le haut pour qu'il regarde vers le haut et se mette \\u00e0 lever la t\\u00eate.\\n\\n2. En m\\u00eame temps, poussez doucement son arri\\u00e8re-train vers l'arri\\u00e8re avec votre autre main et dites \\\"assis\\\" d'une voix ferme et claire.\\n\\n3. Si votre chien s'assoit correctement, r\\u00e9compensez-le avec la friandise et faites beaucoup d'\\u00e9loges pour renforcer le comportement souhait\\u00e9.\\n\\n4. Si votre chien ne s'assoit pas, recommencez les \\u00e9tapes 1-3 jusqu'\\u00e0 ce que vous obtenez le r\\u00e9sultat souhait\\u00e9.\\n\\n5. Pratiquez la commande 'assis' r\\u00e9guli\\u00e8rement avec votre chien jusqu'\\u00e0 ce qu'il la ma\\u00eetrise parfaitement dans toutes les situations.\\n\\nBon apprentissage \\u00e0 vous et votre ami \\u00e0 quatre pattes!\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Hey ChatGPT how can I ask a question to\n",
    "import openai\n",
    "\n",
    "# Set up your OpenAI API credentials\n",
    "openai.api_key = open('openai.txt','r').read()\n",
    "\n",
    "# Define the function to ask a question\n",
    "def ask_chatgpt_question(question):\n",
    "    #prompt = f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    # Generate a response from ChatGPT\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo', # Select the model you want to use\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful AI who always response in French and are funny!\"},\n",
    "            \n",
    "          ],\n",
    "        # temperature=.0\n",
    "    )\n",
    "\n",
    "    # Extract and return the answer from the response\n",
    "    answer = response.choices[0].message\n",
    "    return answer\n",
    "\n",
    "\n",
    "question = \"How to teach a dog the 'sit' command?\"\n",
    "answer = ask_chatgpt_question(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926eaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f98996",
   "metadata": {},
   "source": [
    "## Some more prompting tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e393e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8887c574",
   "metadata": {},
   "source": [
    "# Appendix: The PLM workflow for supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40335ff7",
   "metadata": {},
   "source": [
    "## Get training examples and annotate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd2540b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527f297b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-05-26 13:14:04--  https://bl.iro.bl.uk/downloads/59a8c52f-e0a5-4432-9897-0db8c067627c?locale=en\n",
      "Resolving bl.iro.bl.uk (bl.iro.bl.uk)... 63.35.13.6, 34.250.15.96, 52.213.146.223\n",
      "Connecting to bl.iro.bl.uk (bl.iro.bl.uk)|63.35.13.6|:443... connected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 144694 (141K) [application/zip]\n",
      "Saving to: ‘animacy.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 35% 1.01M 0s\n",
      "    50K .......... .......... .......... .......... .......... 70% 3.00M 0s\n",
      "   100K .......... .......... .......... .......... .         100% 5.30M=0.07s\n",
      "\n",
      "2023-05-26 13:14:04 (1.92 MB/s) - ‘animacy.zip’ saved [144694/144694]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  animacy.zip\n",
      "  inflating: LwM-nlp-animacy-annotations-machines19thC.tsv  \n",
      "  inflating: read-me                 \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget https://bl.iro.bl.uk/downloads/59a8c52f-e0a5-4432-9897-0db8c067627c?locale=en -O animacy.zip \n",
    "unzip animacy.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8d58737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-b6501ae6ef6834b2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbff54ee72a4fe8b42bc23413cc563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d5ae7d0240461582a7f4bf76abfb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-b6501ae6ef6834b2/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24156b5f822d4a1a80e3249f6b16d8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"LwM-nlp-animacy-annotations-machines19thC.tsv\",sep='\\t')\n",
    "dataset = dataset['train']\n",
    "lab2code = {label:i for i,label in enumerate(dataset.unique('animacy'))}\n",
    "num_labels = len(lab2code)\n",
    "dataset = dataset.map(lambda x: {'label': lab2code[x['animacy']]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87cd8ac",
   "metadata": {},
   "source": [
    "## Divide data in training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eaa657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-35bfe5b52d3d2487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a72f9c0614f0d6c9.arrow and /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-35bfe5b52d3d2487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-469104c02e0d42ef.arrow\n",
      "Loading cached split indices for dataset at /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-35bfe5b52d3d2487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e3da498dffb524db.arrow and /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-35bfe5b52d3d2487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f5a589e0070fd7e3.arrow\n"
     ]
    }
   ],
   "source": [
    "test_size = int(len(dataset)*.3)\n",
    "train_test = dataset.train_test_split(test_size=test_size , seed=42)\n",
    "test_set = train_test['test']\n",
    "val_size = int(len(train_test['train'])*.05)\n",
    "train_val =  train_test['train'].train_test_split(test_size=val_size,seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14875b",
   "metadata": {},
   "source": [
    "## Load a Pretrained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43dfa771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc3fc8",
   "metadata": {},
   "source": [
    "## Preprocess data for classification (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9de49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/kasparbeelen/.cache/huggingface/datasets/csv/default-35bfe5b52d3d2487/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b69ae946ebe286ea.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples, target_col):\n",
    "    return tokenizer(examples[target_col], truncation=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "sent_col = 'Sentence'\n",
    "train_val = train_val.map(preprocess_function,fn_kwargs={'target_col': sent_col})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3497e77",
   "metadata": {},
   "source": [
    "## Instantiate a training routine and train model on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6dd78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kasparbeelen/anaconda3/envs/sas-llm/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 02:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.39019122314453125, metrics={'train_runtime': 131.0142, 'train_samples_per_second': 15.113, 'train_steps_per_second': 1.908, 'total_flos': 49649401957200.0, 'train_loss': 0.39019122314453125, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../results\",\n",
    "    seed = 42,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_val[\"train\"],\n",
    "    eval_dataset=train_val[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ec420",
   "metadata": {},
   "source": [
    "## Evaluate on test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aea5e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8370786516853933"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = test_set.map(preprocess_function,fn_kwargs={'target_col': sent_col})\n",
    "predictions = trainer.predict(test_set)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "f1_score(preds,predictions.label_ids,average='binary')\n",
    "f1_score(preds,predictions.label_ids,average='macro')\n",
    "f1_score(preds,predictions.label_ids,average='micro')\n",
    "accuracy_score(preds,predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0350fe",
   "metadata": {},
   "source": [
    "# The model only returns logits by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918caf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.79532534,  0.807309  ],\n",
       "       [ 1.6490573 , -1.8407761 ],\n",
       "       [ 1.3956069 , -1.5528322 ],\n",
       "       [ 1.6038284 , -1.8307273 ],\n",
       "       [ 0.35651925, -0.50323427],\n",
       "       [ 1.578744  , -1.7292355 ],\n",
       "       [ 1.4680144 , -1.7162293 ],\n",
       "       [-0.9894928 ,  0.96960855],\n",
       "       [-0.9246854 ,  0.91035235],\n",
       "       [ 0.881013  , -1.0820259 ],\n",
       "       [-0.91229635,  0.93462545],\n",
       "       [ 1.5856853 , -1.8486041 ],\n",
       "       [ 0.24181026, -0.37124705],\n",
       "       [ 1.4725273 , -1.6652948 ],\n",
       "       [ 1.4842608 , -1.6536903 ],\n",
       "       [ 1.6254208 , -1.9031657 ],\n",
       "       [ 1.4828341 , -1.7547784 ],\n",
       "       [ 1.5872903 , -1.8092145 ],\n",
       "       [ 1.5104885 , -1.6733677 ],\n",
       "       [ 1.5685087 , -1.8136338 ],\n",
       "       [-1.0376576 ,  0.9543897 ],\n",
       "       [ 1.7100545 , -1.8320441 ],\n",
       "       [ 1.5658672 , -1.807062  ],\n",
       "       [-0.837896  ,  0.8632832 ],\n",
       "       [ 1.6709071 , -1.9060299 ],\n",
       "       [-0.34241217,  0.33353895],\n",
       "       [-0.8912124 ,  0.8893115 ],\n",
       "       [-0.85946274,  0.91413367],\n",
       "       [-0.40976182,  0.36927062],\n",
       "       [-0.97456586,  0.9145091 ],\n",
       "       [ 0.04596691, -0.1243021 ],\n",
       "       [-0.23836467,  0.31044763],\n",
       "       [ 0.9887329 , -1.0833663 ],\n",
       "       [ 1.62156   , -1.878321  ],\n",
       "       [-0.8636338 ,  0.7421281 ],\n",
       "       [ 1.4688532 , -1.6922271 ],\n",
       "       [ 0.6501323 , -0.7315091 ],\n",
       "       [ 1.4374477 , -1.4889046 ],\n",
       "       [ 0.10198548, -0.01449908],\n",
       "       [ 0.06662337, -0.14263502],\n",
       "       [-0.78034556,  0.67883706],\n",
       "       [ 1.5874931 , -1.7434572 ],\n",
       "       [ 1.6418179 , -1.8292744 ],\n",
       "       [-0.44493413,  0.48344827],\n",
       "       [ 1.5750312 , -1.768262  ],\n",
       "       [-1.0180427 ,  1.0067637 ],\n",
       "       [ 1.6080146 , -1.8128128 ],\n",
       "       [-0.93440753,  0.9506047 ],\n",
       "       [ 1.4873638 , -1.7963573 ],\n",
       "       [-1.0452958 ,  0.9943727 ],\n",
       "       [ 0.50130075, -0.5719663 ],\n",
       "       [ 1.6084936 , -1.8855052 ],\n",
       "       [ 1.6719339 , -1.9058847 ],\n",
       "       [ 1.5972757 , -1.8658837 ],\n",
       "       [ 1.4580898 , -1.7035757 ],\n",
       "       [ 1.5792379 , -1.8153903 ],\n",
       "       [ 0.03812402, -0.05569162],\n",
       "       [ 1.4310193 , -1.7271101 ],\n",
       "       [-0.7408739 ,  0.70471233],\n",
       "       [ 1.2979529 , -1.4692758 ],\n",
       "       [ 1.3305316 , -1.5915617 ],\n",
       "       [ 0.9242512 , -1.0656846 ],\n",
       "       [ 1.55625   , -1.7601717 ],\n",
       "       [ 1.4015495 , -1.6458554 ],\n",
       "       [ 1.591438  , -1.8867399 ],\n",
       "       [ 1.2520083 , -1.2994727 ],\n",
       "       [ 1.1471496 , -1.194238  ],\n",
       "       [-0.90408134,  0.84265167],\n",
       "       [-1.0301046 ,  0.95476145],\n",
       "       [-0.24914272,  0.32550076],\n",
       "       [-1.0784134 ,  1.0305907 ],\n",
       "       [ 1.6050514 , -1.850208  ],\n",
       "       [-0.23006679,  0.27791446],\n",
       "       [ 1.5452677 , -1.8227748 ],\n",
       "       [-0.9419451 ,  0.93273455],\n",
       "       [ 1.5838374 , -1.8028299 ],\n",
       "       [ 1.4385315 , -1.6784276 ],\n",
       "       [-0.28997472,  0.2928569 ],\n",
       "       [ 1.660019  , -1.8700137 ],\n",
       "       [-0.99854344,  0.94502145],\n",
       "       [ 0.57461536, -0.6639977 ],\n",
       "       [-1.0062251 ,  1.0442568 ],\n",
       "       [-0.5895538 ,  0.6506536 ],\n",
       "       [ 0.7583773 , -0.8839274 ],\n",
       "       [-0.9440235 ,  0.94231975],\n",
       "       [ 1.308557  , -1.4318488 ],\n",
       "       [-0.16808547,  0.07515271],\n",
       "       [ 1.6388497 , -1.8539087 ],\n",
       "       [ 1.5471421 , -1.83045   ],\n",
       "       [ 1.6231612 , -1.7926351 ],\n",
       "       [ 1.4777393 , -1.6960622 ],\n",
       "       [ 0.14985044, -0.20469056],\n",
       "       [ 1.5037637 , -1.6663022 ],\n",
       "       [ 0.32481912, -0.36902902],\n",
       "       [-1.0319574 ,  0.9961576 ],\n",
       "       [ 1.5530225 , -1.8740234 ],\n",
       "       [-0.98556954,  0.95713234],\n",
       "       [-1.0260148 ,  1.0155326 ],\n",
       "       [ 1.4359293 , -1.7190354 ],\n",
       "       [-0.9209269 ,  1.0247848 ],\n",
       "       [ 1.612868  , -1.89377   ],\n",
       "       [-1.0742931 ,  1.022443  ],\n",
       "       [ 0.4775044 , -0.6264713 ],\n",
       "       [-0.98059756,  0.94878924],\n",
       "       [ 1.4844453 , -1.6747468 ],\n",
       "       [ 1.5947409 , -1.9274427 ],\n",
       "       [-0.50409514,  0.45872805],\n",
       "       [ 1.4046953 , -1.6320649 ],\n",
       "       [ 1.5633396 , -1.7994782 ],\n",
       "       [-0.8332403 ,  0.9115436 ],\n",
       "       [-1.045217  ,  0.9731178 ],\n",
       "       [ 1.6304681 , -1.8971573 ],\n",
       "       [ 1.3156562 , -1.3853714 ],\n",
       "       [ 1.6427284 , -1.7842838 ],\n",
       "       [-0.97608083,  0.94226015],\n",
       "       [ 1.5237719 , -1.8147029 ],\n",
       "       [ 1.4398516 , -1.5962671 ],\n",
       "       [ 1.531645  , -1.739298  ],\n",
       "       [ 1.4764403 , -1.6484127 ],\n",
       "       [-0.76300055,  0.66540885],\n",
       "       [-0.8633747 ,  0.86096615],\n",
       "       [ 1.6587416 , -1.8671788 ],\n",
       "       [ 1.5956463 , -1.7769265 ],\n",
       "       [-0.11642972,  0.16030145],\n",
       "       [ 0.18089524, -0.18436466],\n",
       "       [ 1.6508946 , -1.8791485 ],\n",
       "       [ 1.6381824 , -1.8017745 ],\n",
       "       [-0.8496498 ,  0.8910089 ],\n",
       "       [ 1.2998246 , -1.538752  ],\n",
       "       [ 0.06845111, -0.09137217],\n",
       "       [-0.48588833,  0.51613414],\n",
       "       [-0.98650825,  0.9788952 ],\n",
       "       [ 1.5558035 , -1.6932987 ],\n",
       "       [ 0.88494575, -1.1148176 ],\n",
       "       [-0.6752622 ,  0.7564937 ],\n",
       "       [ 0.42885938, -0.5039989 ],\n",
       "       [ 1.4502707 , -1.5651256 ],\n",
       "       [ 1.6507962 , -1.8112409 ],\n",
       "       [ 1.6132629 , -1.762473  ],\n",
       "       [ 1.5982418 , -1.7862239 ],\n",
       "       [-0.7405454 ,  0.71124756],\n",
       "       [ 1.4656612 , -1.7053703 ],\n",
       "       [ 0.9991902 , -1.1040014 ],\n",
       "       [ 1.2729986 , -1.5232947 ],\n",
       "       [-0.99804914,  0.8950645 ],\n",
       "       [-0.52930915,  0.4116611 ],\n",
       "       [ 0.5525656 , -0.557587  ],\n",
       "       [-0.84695804,  0.76209646],\n",
       "       [-0.4778185 ,  0.3491377 ],\n",
       "       [ 1.6273046 , -1.880925  ],\n",
       "       [-1.0013456 ,  0.96490216],\n",
       "       [ 1.5401373 , -1.7480882 ],\n",
       "       [-0.96078783,  1.0168443 ],\n",
       "       [ 1.1516829 , -1.3349473 ],\n",
       "       [ 1.6916856 , -1.879283  ],\n",
       "       [ 1.2035918 , -1.3412242 ],\n",
       "       [-1.0309633 ,  0.9802984 ],\n",
       "       [ 1.642941  , -1.8285276 ],\n",
       "       [-0.89993346,  0.9428462 ],\n",
       "       [-0.17022865,  0.11498935],\n",
       "       [ 1.6086617 , -1.8903397 ],\n",
       "       [ 1.5687834 , -1.8106833 ],\n",
       "       [ 1.583744  , -1.8485779 ],\n",
       "       [ 1.3521856 , -1.4904354 ],\n",
       "       [-0.59684056,  0.5504198 ],\n",
       "       [ 1.496784  , -1.6987804 ],\n",
       "       [ 1.6306925 , -1.9157634 ],\n",
       "       [ 1.33705   , -1.4956963 ],\n",
       "       [ 0.82452404, -0.94754267],\n",
       "       [-0.82074136,  0.71379364],\n",
       "       [-1.0081564 ,  0.9564634 ],\n",
       "       [ 1.4771922 , -1.661548  ],\n",
       "       [ 1.6716911 , -1.9101264 ],\n",
       "       [ 1.6642509 , -1.8962295 ],\n",
       "       [-0.59178257,  0.53545076],\n",
       "       [ 1.6484985 , -1.8743483 ],\n",
       "       [ 1.611657  , -1.8868738 ],\n",
       "       [-0.8973686 ,  0.84735453]], dtype=float32), label_ids=array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1]), metrics={'test_loss': 0.39696288108825684, 'test_runtime': 3.1271, 'test_samples_per_second': 56.921, 'test_steps_per_second': 7.355})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sas-llm",
   "language": "python",
   "name": "sas-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
