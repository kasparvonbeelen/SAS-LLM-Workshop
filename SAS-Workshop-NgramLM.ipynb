{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50f7f68",
   "metadata": {},
   "source": [
    "# Pocking at Ever Larger Language Models\n",
    "### An introduction for (digital) humanists\n",
    "\n",
    "## Part 1: Exploring N-gram language models\n",
    "\n",
    "In this notebook:\n",
    "- We explain some of the basic terminology and principles of language models\n",
    "- Build a simple language model that generates endless (and senseless!) Brexit discourse. Yay!\n",
    "\n",
    "In creating this notebook, I leaned heavily on:\n",
    "- the excellent Programming Historian lesson on [Generating Text with GPT-2](https://programminghistorian.org/assets/interrogating-national-narrative-gpt/)\n",
    "- The [Chapter](https://web.stanford.edu/~jurafsky/slp3/3.pdf) on N-Gram Language Models in the fabulous handbook by Dan Jurafsky and James H. Martin \"Speech and Language Processing\" (3rd ed. draft). Available online [here](https://web.stanford.edu/~jurafsky/slp3/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9a90d",
   "metadata": {},
   "source": [
    "# What are N-Grams?\n",
    "\n",
    "N-grams are sequences of *n* words.\n",
    "\n",
    "The sentence “predicting the future is impossible”\n",
    "- Unigrams: `predicting`,`the `, `future`, `is`, `impossible`\n",
    "- Bigrams:  `predicting the`,`the future`, `future is`, `is impossible`\n",
    "- Trigrams: `predicting the future`, `the future is`, `future is impossible`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570872a",
   "metadata": {},
   "source": [
    "# What are language models\n",
    "## What is likely to come next?\n",
    "\n",
    "> “[Language models] assign a probability* to each possible next word. (Jurafsky & Martin)”\n",
    "\n",
    "In other words, language models guess what word comes next.\n",
    "\n",
    "Given the sentence **“Predicting the future is hard, but not …”**\n",
    "\n",
    "- P(“impossible” | sentence) is greater than P(“aardvark” | sentence)\n",
    "\n",
    "\n",
    "```Read P(“impossible” | sentence) as the probability of observing the token “impossible” given the sequence “Predicting the future is hard, but not ...\"```\n",
    "\n",
    "\n",
    "```Probabilities are values between 0 and 1 that sum up to 1.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ea39c",
   "metadata": {},
   "source": [
    "**Peaking ahead**: if you can predict what comes next in a text sequence you learn quite a lot about language user and the world in general.\n",
    "\n",
    "- Paris is located in [BLANK]\n",
    "- He was late. I was really angry and told [BLANK]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de1892",
   "metadata": {},
   "source": [
    "## How probable is a text?\n",
    "\n",
    "> “[Language models] assign a probability to an entire sequence. (Jurafsky & Martin)”\n",
    "\n",
    "**P(predicting the future is hard, but not impossible)**\n",
    "\n",
    "Is greater than\n",
    "\n",
    "**P(predicting the future is hard, but not aardvark)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8107d0",
   "metadata": {},
   "source": [
    "### How to compute probability of a text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507da2d6",
   "metadata": {},
   "source": [
    "**P(predicting the future is impossible)**\n",
    "\n",
    "Is equal to:\n",
    "\n",
    "`P(predicting)` * `P(the | predicting)` * `P(future | predicting the)` * `P(is | predicting the future)` * `P(impossible | predicting the future is)` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb20d43",
   "metadata": {},
   "source": [
    "# N-Gram Language Models\n",
    "\n",
    "How to compute ```P(impossible | predicting the future is)```? with a **trigram** language model (n=3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967d8b4",
   "metadata": {},
   "source": [
    "- When predicting the next word we only take into account the past two words (Markov Assumption for trigram language model)\n",
    "    i.e. instead of ``P(impossible | predicting the future is)`` we use ``P(impossible | future is)``\n",
    "   \n",
    "- Wow, this is crazy! Imagine you have to make a speech, but you can only remember the last two words.\n",
    "![forgot](https://media.giphy.com/media/dSdvJvsJQIXWsgP6cO/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25184b",
   "metadata": {},
   "source": [
    "Ok, let's assume the Markov assumption makes sense. How to compute ```P(impossible | future is)```\n",
    "\n",
    "You need \n",
    "- [x] lots of data\n",
    "- [x] an N-gram counter\n",
    "- [x] a calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e8366",
   "metadata": {},
   "source": [
    "> The language modelling task: **P(w | h)** (predict the probability of word w given a history h)\n",
    "\n",
    "We can estimate the probability from counts:\n",
    "\n",
    "- P(impossible | predicting the future is) ≈ P(impossible | future is) (Markov assumption)\n",
    "- P(impossible | future is) = C(future is impossible) / C(future is) (Likelihood estimation)\n",
    "\n",
    "Important\n",
    "- By computing the probabilities we **train** the N-gram language model on the actual text.\n",
    "- When we repeat this computation for **every word** in our vocabulary, then get a probability distribution over the next word, i.e.\n",
    "\n",
    "P(impossible | future is) = .05\n",
    "\n",
    "P(not | future is) = .07\n",
    "...\n",
    "\n",
    "P(flower | future is) = .00001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee81af",
   "metadata": {},
   "source": [
    "**Please notice**: Having a higher order language model (i.e. n > 3) will generally create a better model, but it makes the computation more complex. Language is highly variable, and many sequences will never be observed (i.e. the denominator equals zero). Also, you need to keep track of more counts!\n",
    "\n",
    "There are various solutions to this, which we will not discuss here (but you can have a look at this [Chapter](https://web.stanford.edu/~jurafsky/slp3/3.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c926a",
   "metadata": {},
   "source": [
    "## Quick recap\n",
    "- Language modelling is the task of predicting the next word *w* given a history *h* (i.e. P(w | h))\n",
    "- At each step, we can compute the probability over all the following words\n",
    "- We can measure the **performance** of a model by evaluating how well a model can predict the next word (it will assign higher probabilities to actual texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ed776",
   "metadata": {},
   "source": [
    "# Practical: Generating Brexit Prose with a Bigram Language Model\n",
    "\n",
    "In the sections below, we build a simple n-gram model to generate text.\n",
    "\n",
    "- Given an input (or prompt), a language model will output a probability over the next words\n",
    "- We will sample from this distribution to select the next word and repeat the process (ad infinitum if you like;-))\n",
    "\n",
    "Even though the machinery has become way more complicated, these components are still the main building blocks behind generative technologies such as ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9047ad",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "\n",
    "We work in Colab Notebooks, and before we start playing with code we need to install some packages.\n",
    "\n",
    "Below you see a code cell. \n",
    "\n",
    "**Action**: Remove the # at the start of each line and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "8fa5022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O requirements.txt https://raw.githubusercontent.com/kasparvonbeelen/SAS-LLM-Worshop/main/requirements.txt\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a383b545",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1d8f8",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "dd11e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bbc58",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We download the texts from the Programming Historian and save them in a variable with the name 'text'.\n",
    "\n",
    "The inspect its content, we can print the first 100 characters. If you are new to Python, congratulations, you have just run your first small program! Congratulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "c975a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK opposition parties have agreed not to back Boris Johnson's demand for a general election before t\n"
     ]
    }
   ],
   "source": [
    "# download data and save the information in the text variable\n",
    "text = requests.get('https://programminghistorian.org/assets/interrogating-national-narrative-gpt/articles.txt').text\n",
    "# print the first hundred characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ccd7a2",
   "metadata": {},
   "source": [
    "### Processing the documents\n",
    "\n",
    "Each line in this document contains a sentence related to Brexit. \n",
    "\n",
    "To use these texts as input to our language, we need to process them. We want to simplify the input so it's easier to construct a language model.\n",
    "-  lowercase the texts\n",
    "- add a start `<s>` and end `</s>` symbol to each sentence\n",
    "- remove punctuation\n",
    "\n",
    "**Please note**: These steps are not necessarily recommended (they remove valuable information when you want to model a sequence). But for this tutorial, we want to keep things simple and therefore have to cut corners here and there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "1bb83199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end symbol to a sentence\n",
    "format_sentence = lambda s: ['<s>'] + pattern.findall(s) + ['</s>']\n",
    "\n",
    "def process_texts(text: str) -> list:\n",
    "    \"\"\"\n",
    "    A functions that process a text input. This function assumens that lines\n",
    "    correspond with sentences. This function will wrap the sentences with \n",
    "    start and end tags (<s> and </s>) and split the sentence into tokens.\n",
    "    At the ends all sentences are combined into one list of tokens.\n",
    "    \n",
    "    Arguments\n",
    "        text (str): a text document with a sentences on each line (seperated by \\n) \n",
    "    Returns\n",
    "         tokens (list): return the input text as one long sequence of tokens\n",
    "    \"\"\"\n",
    "    pattern = re.compile('\\w+')\n",
    "\n",
    "    text = text.lower()\n",
    "    sentences = [format_sentence(s) for s in text.split('\\n')]\n",
    "    return [w for s in sentences for w in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b356225",
   "metadata": {},
   "source": [
    "The `process_texts` function will prepare our data for training the language model. You can read the docstring to better understand what the function requires as input, how it process this input and what it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "eb8c1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of tokens in our corpus is:  4342340\n",
      "The first 20 tokens are:  ['<s>', 'uk', 'opposition', 'parties', 'have', 'agreed', 'not', 'to', 'back', 'boris', 'johnson', 's', 'demand', 'for', 'a', 'general', 'election', 'before', 'the', 'eu']\n"
     ]
    }
   ],
   "source": [
    "tokens = process_texts(text) # apply process texts to our document\n",
    "print('The total number of tokens in our corpus is: ',len(tokens))\n",
    "print('The first 20 tokens are: ',tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad0884",
   "metadata": {},
   "source": [
    "### Obtain the vocubulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "37ee74a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary contains 39924 distinct words.\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(tokens) # get the vocabulary, all the unique tokens in our corpus\n",
    "print(f'The vocabulary contains {len(vocabulary)} distinct words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c6cdd",
   "metadata": {},
   "source": [
    "After processing the input data, we can start building our language model using n-gram counts. As explained in the introduction, need to compute the P(w | h), probabily of observing word w given history h.  We approximate this with bigram language model, which uses the relative counts for calculating the likelihood of observing w after h. \n",
    "\n",
    "Below we created a function that returns n-grams of length n given a sequence (of tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1bb1f",
   "metadata": {},
   "source": [
    "### Compute N-Gram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "04a8e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens: list,n: int=2):\n",
    "    \"\"\"\n",
    "    A function the returns ngrams with length n as a list\n",
    "    \n",
    "    Arguments:\n",
    "        tokens (list): a list of tokens processed \n",
    "        with the process_texts function\n",
    "        n (int): an integer that determines the length of the n-grams\n",
    "    Returns:\n",
    "        a list in which each elements is n-gram of tokens represented as a string\n",
    "    \"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27702eb7",
   "metadata": {},
   "source": [
    "We can start with the simplest example of bigrams, sequences of length two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "624c8ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> uk',\n",
       " 'uk opposition',\n",
       " 'opposition parties',\n",
       " 'parties have',\n",
       " 'have agreed',\n",
       " 'agreed not',\n",
       " 'not to',\n",
       " 'to back',\n",
       " 'back boris',\n",
       " 'boris johnson']"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_list = ngrams(tokens,2) # compute bigrams\n",
    "bigrams_list[:10] # print first 10 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82505f3b",
   "metadata": {},
   "source": [
    "As we need the n-grams counts for computing the likelihood, we need feed this list of bigrams to a `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "cf202708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('</s> <s>', 130568), ('of the', 25933), ('the eu', 20235), ('in the', 20000), ('<s> the', 17367)]\n"
     ]
    }
   ],
   "source": [
    "bigrams = Counter(bigrams_list) # compute bigram counts\n",
    "print(bigrams.most_common(5)) # print 5 most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "76cbdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Counter(ngrams(tokens,3)) # create n-grams of length 3\n",
    "quadgrams = Counter(ngrams(tokens,4)) # create n-grams of length 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "e656964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([w for w,v in trigrams.most_common()][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "09d8afcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print([w for w,v in trigrams.most_common() if w.startswith('<s>')][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c8e3f",
   "metadata": {},
   "source": [
    "### Compute the probabilities over the next word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff3f19",
   "metadata": {},
   "source": [
    "Let's now compute, step by step, the next word for an input sequence. We use the trigram \"a no deal\" as prompt, and calculate what tokens is likely to follow.\n",
    "\n",
    "How likely is 'brexit' to follow 'a no deal'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "4fca8bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of \"brexit\" following \"a no deal\" is 0.646\n"
     ]
    }
   ],
   "source": [
    "sequence = 'a no deal' # prompt or history\n",
    "w = 'brexit' # next word w\n",
    "probability = quadgram[f'{sequence} {w}'] / trigrams[sequence] # compute P(h | w)\n",
    "print(f'The probability of \"{w}\" following \"{sequence}\" is {probability:.3f}') # print result of computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130f0b9",
   "metadata": {},
   "source": [
    "However, we need to calculate these probabilities for each word in our vocabulary. Luckily we have machines to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "62c445d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute P(w | h) for each word w in our vocabulary\n",
    "prob_next_word = Counter({w: quadgram[f'{sequence} {w}'] / trigrams[sequence] for w in vocabulary}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "df091a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brexit', 0.64616782793093),\n",
       " ('exit', 0.07906694940926992),\n",
       " ('scenario', 0.06846410178733717),\n",
       " ('outcome', 0.028476219327476522),\n",
       " ('</s>', 0.025446834292638595),\n",
       " ('departure', 0.016964556195092396),\n",
       " ('and', 0.006664647076643442),\n",
       " ('situation', 0.005755831566192063),\n",
       " ('would', 0.005755831566192063),\n",
       " ('or', 0.00545289306270827)]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print ten most probably words that can follow the input\n",
    "prob_next_word.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e83492",
   "metadata": {},
   "source": [
    "We can visualize the distribution of these probabilities using a bar chart. We only focus on the twenty words with the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "d8f47f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'probability of next word'}>"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_prob = pd.DataFrame(list(prob_next_word.most_common(20)),columns=['token','prob'])\n",
    "df_prob.index = list(df_prob[\"token\"])\n",
    "df_prob.plot(kind='bar', title='probability of next word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7743f6",
   "metadata": {},
   "source": [
    "### Sample from the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fdbc2",
   "metadata": {},
   "source": [
    "After computing the probabilities, we sample the next word. This adds a stochastic element to language models, i.e. outputs may differ each time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "cabd785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, probs = list(prob_next_word),list(prob_next_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "e4998afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brexit'"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(words, p=probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67eedc",
   "metadata": {},
   "source": [
    "While words with a high probability are more likely to be selected, the outcome differs on each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "2605319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brexit\n",
      "outcome\n",
      "exit\n",
      "</s>\n",
      "scenario\n",
      "brexit\n",
      "withdrawal\n",
      "brexit\n",
      "exit\n",
      "brexit\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(np.random.choice(words, p=probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce00d0",
   "metadata": {},
   "source": [
    "### Generate text from a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "34f470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(sequence: str) -> str:\n",
    "    \"\"\"\n",
    "    Given an input or prompt, keep on generating text until \n",
    "    you encounter the end-of-sentence token.\n",
    "    \n",
    "    Arguments:\n",
    "        sequence (str): input sequence or prompt\n",
    "    \n",
    "    Returns:\n",
    "        a generated text sequence \n",
    "    \"\"\"\n",
    "    \n",
    "    total_sequence = sequence # to keep track of each prediction we create a variable with the name total_sequence\n",
    "\n",
    "    next_word = None # we need to define the next word, but assign it a None value\n",
    "\n",
    "    while next_word != '</s>': # we keep generating text untill we encounter the end of sentence token\n",
    "        prob_next_word = Counter({w: quadgram[f'{sequence} {w}'] / trigrams[sequence] for w in vocabulary}) # compute probabilities\n",
    "        words, probs = list(prob_next_word),list(prob_next_word.values()) # compute probabilities\n",
    "        next_word = np.random.choice(words, p=probs) # sample\n",
    "        total_sequence += ' ' + next_word # add sampled word to sequence\n",
    "        sequence = ' '.join(total_sequence.split()[-3:]) # change the history to include to last three words\n",
    "    return total_sequence # return sequence after encountering a stop symbol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "6fec6d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> theresa may entered office three years ago mrs may pledges to fight burning injustices in society here she lists her proudest achievements mental health funding domestic abuse an audit of racial discrimination and gender pay gap and organic food he was also a remainer in his heart the shadow chancellor wants to postpone as many serious decisions as possible until next year the uk should strike with the eu27 and the uk ended up contesting european elections </s>'"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_sequence = '<s> theresa may' # select a sequnence as a prompt to start generating texts\n",
    "#start_sequence = '<s> boris johnson'\n",
    "generate_text(start_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6e2bb",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sas-llm",
   "language": "python",
   "name": "sas-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
